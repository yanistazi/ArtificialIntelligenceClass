{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training a Decision Tree or a Random Forest on a classification problem, and compare the latter with using adaBoost\n",
    "\n",
    "**Author: Pr Fabien MOUTARDE, Center for Robotics, MINES ParisTech, PSL Universit√© Paris**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Trees with SciKit-Learn on a very simple dataset\n",
    "\n",
    "**We will first work on very simple classic dataset: Iris, which is a classification problem corresponding to determination of iris flower sub-species based on a few geometric characteristics of the flower.**\n",
    "\n",
    "**Please FIRST READ the [*Iris DATASET DESCRIPTION*](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html#sphx-glr-auto-examples-datasets-plot-iris-dataset-py).**\n",
    "In this classification problem, there are 3 classes, with a total of 150 examples (each one with 4 input). Please **now execute code cell below to load and view the dataset**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Number_of_examples, example_size) =  (150, 4) \n\nInput =  [5.1 3.5 1.4 0.2]  , Label =  0\nInput =  [4.9 3.  1.4 0.2]  , Label =  0\nInput =  [4.7 3.2 1.3 0.2]  , Label =  0\nInput =  [4.6 3.1 1.5 0.2]  , Label =  0\nInput =  [5.  3.6 1.4 0.2]  , Label =  0\nInput =  [5.4 3.9 1.7 0.4]  , Label =  0\nInput =  [4.6 3.4 1.4 0.3]  , Label =  0\nInput =  [5.  3.4 1.5 0.2]  , Label =  0\nInput =  [4.4 2.9 1.4 0.2]  , Label =  0\nInput =  [4.9 3.1 1.5 0.1]  , Label =  0\nInput =  [5.4 3.7 1.5 0.2]  , Label =  0\nInput =  [4.8 3.4 1.6 0.2]  , Label =  0\nInput =  [4.8 3.  1.4 0.1]  , Label =  0\nInput =  [4.3 3.  1.1 0.1]  , Label =  0\nInput =  [5.8 4.  1.2 0.2]  , Label =  0\nInput =  [5.7 4.4 1.5 0.4]  , Label =  0\nInput =  [5.4 3.9 1.3 0.4]  , Label =  0\nInput =  [5.1 3.5 1.4 0.3]  , Label =  0\nInput =  [5.7 3.8 1.7 0.3]  , Label =  0\nInput =  [5.1 3.8 1.5 0.3]  , Label =  0\nInput =  [5.4 3.4 1.7 0.2]  , Label =  0\nInput =  [5.1 3.7 1.5 0.4]  , Label =  0\nInput =  [4.6 3.6 1.  0.2]  , Label =  0\nInput =  [5.1 3.3 1.7 0.5]  , Label =  0\nInput =  [4.8 3.4 1.9 0.2]  , Label =  0\nInput =  [5.  3.  1.6 0.2]  , Label =  0\nInput =  [5.  3.4 1.6 0.4]  , Label =  0\nInput =  [5.2 3.5 1.5 0.2]  , Label =  0\nInput =  [5.2 3.4 1.4 0.2]  , Label =  0\nInput =  [4.7 3.2 1.6 0.2]  , Label =  0\nInput =  [4.8 3.1 1.6 0.2]  , Label =  0\nInput =  [5.4 3.4 1.5 0.4]  , Label =  0\nInput =  [5.2 4.1 1.5 0.1]  , Label =  0\nInput =  [5.5 4.2 1.4 0.2]  , Label =  0\nInput =  [4.9 3.1 1.5 0.2]  , Label =  0\nInput =  [5.  3.2 1.2 0.2]  , Label =  0\nInput =  [5.5 3.5 1.3 0.2]  , Label =  0\nInput =  [4.9 3.6 1.4 0.1]  , Label =  0\nInput =  [4.4 3.  1.3 0.2]  , Label =  0\nInput =  [5.1 3.4 1.5 0.2]  , Label =  0\nInput =  [5.  3.5 1.3 0.3]  , Label =  0\nInput =  [4.5 2.3 1.3 0.3]  , Label =  0\nInput =  [4.4 3.2 1.3 0.2]  , Label =  0\nInput =  [5.  3.5 1.6 0.6]  , Label =  0\nInput =  [5.1 3.8 1.9 0.4]  , Label =  0\nInput =  [4.8 3.  1.4 0.3]  , Label =  0\nInput =  [5.1 3.8 1.6 0.2]  , Label =  0\nInput =  [4.6 3.2 1.4 0.2]  , Label =  0\nInput =  [5.3 3.7 1.5 0.2]  , Label =  0\nInput =  [5.  3.3 1.4 0.2]  , Label =  0\nInput =  [7.  3.2 4.7 1.4]  , Label =  1\nInput =  [6.4 3.2 4.5 1.5]  , Label =  1\nInput =  [6.9 3.1 4.9 1.5]  , Label =  1\nInput =  [5.5 2.3 4.  1.3]  , Label =  1\nInput =  [6.5 2.8 4.6 1.5]  , Label =  1\nInput =  [5.7 2.8 4.5 1.3]  , Label =  1\nInput =  [6.3 3.3 4.7 1.6]  , Label =  1\nInput =  [4.9 2.4 3.3 1. ]  , Label =  1\nInput =  [6.6 2.9 4.6 1.3]  , Label =  1\nInput =  [5.2 2.7 3.9 1.4]  , Label =  1\nInput =  [5.  2.  3.5 1. ]  , Label =  1\nInput =  [5.9 3.  4.2 1.5]  , Label =  1\nInput =  [6.  2.2 4.  1. ]  , Label =  1\nInput =  [6.1 2.9 4.7 1.4]  , Label =  1\nInput =  [5.6 2.9 3.6 1.3]  , Label =  1\nInput =  [6.7 3.1 4.4 1.4]  , Label =  1\nInput =  [5.6 3.  4.5 1.5]  , Label =  1\nInput =  [5.8 2.7 4.1 1. ]  , Label =  1\nInput =  [6.2 2.2 4.5 1.5]  , Label =  1\nInput =  [5.6 2.5 3.9 1.1]  , Label =  1\nInput =  [5.9 3.2 4.8 1.8]  , Label =  1\nInput =  [6.1 2.8 4.  1.3]  , Label =  1\nInput =  [6.3 2.5 4.9 1.5]  , Label =  1\nInput =  [6.1 2.8 4.7 1.2]  , Label =  1\nInput =  [6.4 2.9 4.3 1.3]  , Label =  1\nInput =  [6.6 3.  4.4 1.4]  , Label =  1\nInput =  [6.8 2.8 4.8 1.4]  , Label =  1\nInput =  [6.7 3.  5.  1.7]  , Label =  1\nInput =  [6.  2.9 4.5 1.5]  , Label =  1\nInput =  [5.7 2.6 3.5 1. ]  , Label =  1\nInput =  [5.5 2.4 3.8 1.1]  , Label =  1\nInput =  [5.5 2.4 3.7 1. ]  , Label =  1\nInput =  [5.8 2.7 3.9 1.2]  , Label =  1\nInput =  [6.  2.7 5.1 1.6]  , Label =  1\nInput =  [5.4 3.  4.5 1.5]  , Label =  1\nInput =  [6.  3.4 4.5 1.6]  , Label =  1\nInput =  [6.7 3.1 4.7 1.5]  , Label =  1\nInput =  [6.3 2.3 4.4 1.3]  , Label =  1\nInput =  [5.6 3.  4.1 1.3]  , Label =  1\nInput =  [5.5 2.5 4.  1.3]  , Label =  1\nInput =  [5.5 2.6 4.4 1.2]  , Label =  1\nInput =  [6.1 3.  4.6 1.4]  , Label =  1\nInput =  [5.8 2.6 4.  1.2]  , Label =  1\nInput =  [5.  2.3 3.3 1. ]  , Label =  1\nInput =  [5.6 2.7 4.2 1.3]  , Label =  1\nInput =  [5.7 3.  4.2 1.2]  , Label =  1\nInput =  [5.7 2.9 4.2 1.3]  , Label =  1\nInput =  [6.2 2.9 4.3 1.3]  , Label =  1\nInput =  [5.1 2.5 3.  1.1]  , Label =  1\nInput =  [5.7 2.8 4.1 1.3]  , Label =  1\nInput =  [6.3 3.3 6.  2.5]  , Label =  2\nInput =  [5.8 2.7 5.1 1.9]  , Label =  2\nInput =  [7.1 3.  5.9 2.1]  , Label =  2\nInput =  [6.3 2.9 5.6 1.8]  , Label =  2\nInput =  [6.5 3.  5.8 2.2]  , Label =  2\nInput =  [7.6 3.  6.6 2.1]  , Label =  2\nInput =  [4.9 2.5 4.5 1.7]  , Label =  2\nInput =  [7.3 2.9 6.3 1.8]  , Label =  2\nInput =  [6.7 2.5 5.8 1.8]  , Label =  2\nInput =  [7.2 3.6 6.1 2.5]  , Label =  2\nInput =  [6.5 3.2 5.1 2. ]  , Label =  2\nInput =  [6.4 2.7 5.3 1.9]  , Label =  2\nInput =  [6.8 3.  5.5 2.1]  , Label =  2\nInput =  [5.7 2.5 5.  2. ]  , Label =  2\nInput =  [5.8 2.8 5.1 2.4]  , Label =  2\nInput =  [6.4 3.2 5.3 2.3]  , Label =  2\nInput =  [6.5 3.  5.5 1.8]  , Label =  2\nInput =  [7.7 3.8 6.7 2.2]  , Label =  2\nInput =  [7.7 2.6 6.9 2.3]  , Label =  2\nInput =  [6.  2.2 5.  1.5]  , Label =  2\nInput =  [6.9 3.2 5.7 2.3]  , Label =  2\nInput =  [5.6 2.8 4.9 2. ]  , Label =  2\nInput =  [7.7 2.8 6.7 2. ]  , Label =  2\nInput =  [6.3 2.7 4.9 1.8]  , Label =  2\nInput =  [6.7 3.3 5.7 2.1]  , Label =  2\nInput =  [7.2 3.2 6.  1.8]  , Label =  2\nInput =  [6.2 2.8 4.8 1.8]  , Label =  2\nInput =  [6.1 3.  4.9 1.8]  , Label =  2\nInput =  [6.4 2.8 5.6 2.1]  , Label =  2\nInput =  [7.2 3.  5.8 1.6]  , Label =  2\nInput =  [7.4 2.8 6.1 1.9]  , Label =  2\nInput =  [7.9 3.8 6.4 2. ]  , Label =  2\nInput =  [6.4 2.8 5.6 2.2]  , Label =  2\nInput =  [6.3 2.8 5.1 1.5]  , Label =  2\nInput =  [6.1 2.6 5.6 1.4]  , Label =  2\nInput =  [7.7 3.  6.1 2.3]  , Label =  2\nInput =  [6.3 3.4 5.6 2.4]  , Label =  2\nInput =  [6.4 3.1 5.5 1.8]  , Label =  2\nInput =  [6.  3.  4.8 1.8]  , Label =  2\nInput =  [6.9 3.1 5.4 2.1]  , Label =  2\nInput =  [6.7 3.1 5.6 2.4]  , Label =  2\nInput =  [6.9 3.1 5.1 2.3]  , Label =  2\nInput =  [5.8 2.7 5.1 1.9]  , Label =  2\nInput =  [6.8 3.2 5.9 2.3]  , Label =  2\nInput =  [6.7 3.3 5.7 2.5]  , Label =  2\nInput =  [6.7 3.  5.2 2.3]  , Label =  2\nInput =  [6.3 2.5 5.  1.9]  , Label =  2\nInput =  [6.5 3.  5.2 2. ]  , Label =  2\nInput =  [6.2 3.4 5.4 2.3]  , Label =  2\nInput =  [5.9 3.  5.1 1.8]  , Label =  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris classification dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Print all 150 examples\n",
    "print(\"(Number_of_examples, example_size) = \" , iris.data.shape, \"\\n\")\n",
    "for i in range(0, 150) :\n",
    "    print('Input = ', iris.data[i], ' , Label = ', iris.target[i] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building, training and evaluating a simple Decision Tree classifier**\n",
    "\n",
    "The SciKit-learn class for Decision Tree classifiers is sklearn.tree.DecisionTreeClassifier.\n",
    "\n",
    "**Please FIRST READ (and understand!) the [*DecisionTreeClassifier DOCUMENTATION*](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) to understand all parameters of the contructor.**\n",
    "\n",
    "**You can then begin by running the code block below, in which default set of parameter values has been used.** If graphical view works, look at the structure of the learnt decision tree.\n",
    "\n",
    "**Then, check the influence of MAIN parameters for Decision Tree classifier, i.e.:**\n",
    " - **homegeneity criterion ('gini' or 'entropy')**\n",
    " - **max_depth**\n",
    " - **min_samples_split**\n",
    " \n",
    "NB : Note that post-training *PRUNING* IS unfortunately *NOT* implemented in SciKit-Learn Decision-Trees :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=1e-07,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n            splitter='best')\nAcuracy (on test set) =  0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanis/venv/ApprentissageArtificiel/lib/python3.6/site-packages/sklearn/tree/tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.metrics import confusion_matrix\\ny_true, y_pred = y_test, clf.predict(X_test)\\nprint( classification_report(y_true, y_pred) )\\nprint(\"\\n CONFUSION MATRIX\")\\nprint( confusion_matrix(y_true, y_pred) )\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training and test part\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n",
    "\n",
    "# Learn a Decision Tree\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=5, \n",
    "                                  min_samples_split=2, min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                  random_state=42, max_leaf_nodes=None, \n",
    "                                  min_impurity_split=1e-07, class_weight=None, presort=False)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Graphical view of learnt Decision Tree\n",
    "#\n",
    "'''\n",
    "import pydotplus \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "graph.write_pdf(\"iris.pdf\")\n",
    "from IPython.display import Image \n",
    "Image(graph.create_png()) \n",
    "'''\n",
    "# Evaluate acuracy on test data\n",
    "print(clf)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Acuracy (on test set) = \", score)\n",
    "'''\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print( classification_report(y_true, y_pred) )\n",
    "print(\"\\n CONFUSION MATRIX\")\n",
    "print( confusion_matrix(y_true, y_pred) )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Trees on a  MORE REALISTIC DATASET: HANDWRITTEN DIGITS\n",
    "\n",
    "**Please FIRST READ the [*Digits DATASET DESCRIPTION*](http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html#sphx-glr-auto-examples-datasets-plot-digits-last-image-py).**\n",
    "\n",
    "In this classification problem, there are 10 classes, with a total of 1797 examples (each one being a 64D vector corresponding to an 8x8 pixmap). Please **now execute code cell below to load the dataset, visualize a typical example, and train a Desicion Tree on it**. \n",
    "The original code uses a **SUBOPTIMAL set of learning hyperparameters values. Try to play with them in order to improve acuracy.**\n",
    "\n",
    "Finally, **find a somewhat optimized setting of the set of 3 main hyper-parameters for Decision Tree learning, by using CROSS-VALIDATION** (see cross-validation example from the Multi-Layer Perceptron notebook used in earlier practical session).\n",
    "\n",
    "Look at final acuracy statistics, and also at the confusion-matrix: what digits are the most confused with each other ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number_of-examples =  1797\n\n Plot of first example\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n\n{'criterion': 'gini', 'max_depth': 9, 'min_samples_split': 2}\n0.8420467185761957\nGridSearchCV(cv=3, error_score='raise-deprecating',\n       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=1e-07,\n            min_samples_leaf=1, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best'),\n       fit_params=None, iid='warn', n_jobs=None,\n       param_grid=[{'criterion': ['gini', 'entropy'], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], 'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16, 18]}],\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=0)\nAcuracy (on test set) =  0.8420467185761957\n              precision    recall  f1-score   support\n\n           0       0.99      0.90      0.94        83\n           1       0.82      0.84      0.83        86\n           2       0.85      0.82      0.83        95\n           3       0.75      0.86      0.80        90\n           4       0.90      0.91      0.90        96\n           5       0.92      0.86      0.89        99\n           6       0.97      0.86      0.91        98\n           7       0.80      0.86      0.83        93\n           8       0.73      0.66      0.69        80\n           9       0.72      0.84      0.77        79\n\n   micro avg       0.84      0.84      0.84       899\n   macro avg       0.84      0.84      0.84       899\nweighted avg       0.85      0.84      0.84       899\n\n\n CONFUSION MATRIX\n[[75  0  2  0  1  1  0  1  3  0]\n [ 0 72  6  0  2  0  0  2  1  3]\n [ 0  1 78  2  0  1  0  5  6  2]\n [ 0  3  4 77  0  0  1  2  3  0]\n [ 0  1  0  2 87  0  1  4  0  1]\n [ 0  1  0  3  1 85  1  1  3  4]\n [ 1  2  0  0  3  4 84  0  1  3]\n [ 0  0  0  5  3  1  0 80  0  4]\n [ 0  7  1  6  0  0  0  4 53  9]\n [ 0  1  1  7  0  0  0  1  3 66]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "digits = load_digits()\n",
    "n_samples = len(digits.images)\n",
    "print(\"Number_of-examples = \", n_samples)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"\\n Plot of first example\")\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) \n",
    "plt.show() \n",
    "\n",
    "# Flatten the images, to turn data in a (samples, feature) matrix:\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Split dataset into training and test part\n",
    "X = data\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# Create and train a Decision Tree Classifier\n",
    "'''\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=10, \n",
    "                                  min_samples_split=10, min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                  random_state=42, max_leaf_nodes=None, \n",
    "                                  min_impurity_split=1e-07, class_weight=None, presort=False)\n",
    "'''\n",
    "param_grid = [\n",
    "    {'criterion': ['gini', 'entropy'], \n",
    "   'max_depth':[i for i in range(1, 25, 1)],\n",
    "   'min_samples_split': [i for i in range(2, 20, 2)]\n",
    "     }]\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier( splitter='best', \n",
    "                                   min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                  random_state=None, max_leaf_nodes=None, \n",
    "                                  min_impurity_split=1e-07, class_weight=None, presort=False), \n",
    "                                  param_grid, cv=3)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print (clf.score(X_test,y_test))\n",
    "print (clf)\n",
    "'''\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=11, \n",
    "                                  min_samples_split=6, min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                  random_state=None, max_leaf_nodes=None, \n",
    "                                  min_impurity_split=1e-07, class_weight=None, presort=False)\n",
    "#clf = clf.fit(X_train, y_train)\n",
    "#print(clf.score(X_test,y_test))\n",
    "# Evaluate acuracy on test data\n",
    "'''\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Acuracy (on test set) = \", score)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print( classification_report(y_true, y_pred) )\n",
    "print(\"\\n CONFUSION MATRIX\")\n",
    "print( confusion_matrix(y_true, y_pred) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building, training and evaluating a Random Forest classifier\n",
    "\n",
    "The SciKit-learn class for Random Forest classifiers is Please sklearn.ensemble.RandomForestClassifier.\n",
    "\n",
    "**Please FIRST READ (and understand!) the [*RandomForestClassifier DOCUMENTATION*](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to understand all parameters of the contructor.**\n",
    "\n",
    "**Then you can begin by running the code block below, in which default set of parameter values has been used.** As you will see, a RandomForest (even rather small) can easily outperform single Decision Tree. \n",
    "\n",
    "**Then, check the influence of MAIN parameters for Random Forest classifier, i.e.:**\n",
    " - **n_estimators (number of trees in forest)**\n",
    " - **max_depth**\n",
    " - **max_features (max number of features used in each tree)**\n",
    "\n",
    "**Finally, find a somewhat optimized setting of the above set of 3 main parameters, by using CROSS-VALIDATION.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=3, error_score='raise-deprecating',\n       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=1e-07,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n       fit_params=None, iid='warn', n_jobs=None,\n       param_grid=[{'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], 'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9]}],\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=0)\nAcuracy (on test set) =  0.9755283648498332\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98        83\n           1       0.97      1.00      0.98        86\n           2       1.00      1.00      1.00        95\n           3       0.99      0.96      0.97        90\n           4       0.98      0.99      0.98        96\n           5       1.00      0.95      0.97        99\n           6       1.00      0.96      0.98        98\n           7       0.96      1.00      0.98        93\n           8       0.95      0.96      0.96        80\n           9       0.93      0.96      0.94        79\n\n   micro avg       0.98      0.98      0.98       899\n   macro avg       0.97      0.98      0.97       899\nweighted avg       0.98      0.98      0.98       899\n\n\n CONFUSION MATRIX\n[[81  0  0  0  2  0  0  0  0  0]\n [ 0 86  0  0  0  0  0  0  0  0]\n [ 0  0 95  0  0  0  0  0  0  0]\n [ 0  0  0 86  0  0  0  2  1  1]\n [ 0  0  0  0 95  0  0  0  0  1]\n [ 0  0  0  0  0 94  0  0  1  4]\n [ 2  1  0  0  0  0 94  0  1  0]\n [ 0  0  0  0  0  0  0 93  0  0]\n [ 0  2  0  0  0  0  0  1 77  0]\n [ 0  0  0  1  0  0  0  1  1 76]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = [\n",
    "    {'n_estimators': [i for i in range(50,500,50)], \n",
    "   'max_depth':[i for i in range(1, 25, 1)],\n",
    "   'max_features': [i for i in range(1, 10)]\n",
    "     }]\n",
    "clf = GridSearchCV(RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "                             min_samples_split=2, min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                             max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, \n",
    "                             oob_score=False, n_jobs=1, random_state=None, \n",
    "                             verbose=0, warm_start=False, class_weight=None), \n",
    "                                  param_grid, cv=3)\n",
    "# Create and train a Random Forest classifier\n",
    "'''\n",
    "clf = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None,\n",
    "                             min_samples_split=2, min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                             max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, \n",
    "                             oob_score=False, n_jobs=1, random_state=None, \n",
    "                             verbose=0, warm_start=False, class_weight=None)\n",
    "'''\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(\"n_estimators=\", clf.n_estimators, \" max_depth=\",clf.max_depth,\n",
    "      \"max_features=\", clf.max_features)\n",
    "\n",
    "# Evaluate acuracy on test data\n",
    "print(clf)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Acuracy (on test set) = \", score)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print( classification_report(y_true, y_pred) )\n",
    "print(\"\\n CONFUSION MATRIX\")\n",
    "print(confusion_matrix(y_true, y_pred) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Building, training and evaluating an AdaBoost classifier\n",
    "\n",
    "The SciKit-learn class for adaBoost is sklearn.ensemble.AdaBoostClassifier.\n",
    "\n",
    "**Please FIRST READ (and understand!) the [*AdaBoostClassifier DOCUMENTATION*](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) to understand all parameters of the contructor.**\n",
    "\n",
    "**Then begin by running the code block below, in which a default set of parameter values has been used.** Look at the training curve: you can see that **training error goes down to zero rather quickly, and that test_error continues to diminish with increasing iterations**.\n",
    "\n",
    "**Then, check the influence of MAIN parameters for adaBoost classifier, i.e.:**\n",
    " - **base_estimator (ie type of Weak Classifier/Learner)** \n",
    " - **n_estimators (number of boosting iterations, and therefore also number of weak classifiers)**\n",
    " - algorithm\n",
    "\n",
    "**First, for the case of DecisionTree weak classifiers, find somewhat optimized values of (max_depth, n_estimators) by using CROSS-VALIDATION.**\n",
    "\n",
    "**Finally, check which other types of classifiers can be used as Weak Classifier with the adaBoost implementation of SciKit-Learn.**\n",
    "\n",
    "NB: in principle it is possible to use MLP classifiers as weak classifiers, but not with SciKit-learn implementation of MLPClassifier (because weighting of examples is not handled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create and train an adaBoost classifier using SMALL Decision Trees as weak classifiers\n",
    "weak_learner = tree.DecisionTreeClassifier(max_depth=6)\n",
    "clf = AdaBoostClassifier(weak_learner, n_estimators=15, learning_rate=1.0, algorithm='SAMME', \n",
    "                         random_state=None)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(\"Weak_learner:\", clf.base_estimator)\n",
    "print(\"Weights of weak classifiers: \", clf.estimator_weights_)\n",
    "      \n",
    "# Plot training curves (error = f(iterations))\n",
    "n_iter = clf.n_estimators\n",
    "from sklearn.metrics import zero_one_loss\n",
    "ada_train_err = np.zeros((clf.n_estimators,))\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_train)):\n",
    "    ada_train_err[i] = zero_one_loss(y_pred, y_train)\n",
    "ada_test_err = np.zeros((clf.n_estimators,))\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "    ada_test_err[i] = zero_one_loss(y_pred, y_test)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(n_iter) + 1, ada_train_err,\n",
    "        label='Training Error',\n",
    "        color='green')\n",
    "ax.plot(np.arange(n_iter) + 1, ada_test_err,\n",
    "        label='Test Error',\n",
    "        color='orange')\n",
    "ax.set_ylim((0.0, 0.5))\n",
    "ax.set_xlabel('boosting iterations')\n",
    "ax.set_ylabel('error rate')\n",
    "leg = ax.legend(loc='upper right', fancybox=True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate acuracy on test data\n",
    "print(\"n_estimators=\", clf.n_estimators)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Acuracy (on test set) = \", score)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print( classification_report(y_true, y_pred) )\n",
    "print(\"\\n CONFUSION MATRIX\")\n",
    "print( confusion_matrix(y_true, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
